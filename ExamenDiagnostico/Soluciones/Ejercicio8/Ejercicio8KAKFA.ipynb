{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6183fe9-b453-46a7-96c7-6a1c6362cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e86dfc9-778d-4dcd-b477-b4bf9a8d3448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74038284-7c02-4c7c-bf73-5bc420d630cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/26 12:29:30 WARN Utils: Your hostname, Macbook-Air-31.local resolves to a loopback address: 127.0.0.1; using 192.168.68.102 instead (on interface en0)\n",
      "24/07/26 12:29:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/enriquegomeztagle/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/enriquegomeztagle/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-96830436-ed36-4ca7-92d0-7e3c1daf3b91;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/enriquegomeztagle/anaconda3/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 172ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-96830436-ed36-4ca7-92d0-7e3c1daf3b91\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/5ms)\n",
      "24/07/26 12:29:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Kafka Streaming Example\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start Spark with Kafka connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146582e9-7e0c-4977-adad-5f0e017de89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Gender\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638215fa-1152-4f76-a7e2-1f6e54bc0baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"test-topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Read data from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc272129-0a49-4723-b2b8-6e114c10db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df_string = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "# Convert value from bytes to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6651c0-0ac6-4a31-8231-374f7d6af290",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = kafka_df_string.select(from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "# Parse JSON and apply schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9874d2a-08b2-4dc7-92d5-6134da7d08d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema del DataFrame en streaming:\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Esquema del DataFrame en streaming:\")\n",
    "parsed_df.printSchema()\n",
    "# Show DF Schema on streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0b11d-9385-4a5d-b0ee-6003c518719a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/26 12:29:32 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/73/m50h3qv91_n45vnh3sfw1pb00000gq/T/temporary-4f73f36c-f966-4a6d-b4d2-42812a7b0257. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/07/26 12:29:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/07/26 12:29:32 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "24/07/26 12:29:32 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "24/07/26 12:29:32 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "24/07/26 12:29:32 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "24/07/26 12:29:32 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----+------+\n",
      "|   Name| Age|Gender|\n",
      "+-------+----+------+\n",
      "|   NULL|NULL|  NULL|\n",
      "|  Alice|  25|     F|\n",
      "|    Bob|  30|     M|\n",
      "|Charlie|  28|     M|\n",
      "|ChDavid|  30|     M|\n",
      "|    Bob|  30|     M|\n",
      "|   Lion|  80|     A|\n",
      "|   Lion|  80|     P|\n",
      "|   Lion|  80|     W|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----+---+------+\n",
      "|Name|Age|Gender|\n",
      "+----+---+------+\n",
      "| Bob| 30|    M2|\n",
      "+----+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/26 12:30:05 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "24/07/26 12:30:05 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "24/07/26 12:30:05 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "24/07/26 12:30:06 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "24/07/26 12:30:07 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n"
     ]
    }
   ],
   "source": [
    "query = parsed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "# View Straming DF on Console\n",
    "\n",
    "query.awaitTermination()\n",
    "# Keep Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8240e-7f9e-4489-9bc0-cf2cffd179a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
